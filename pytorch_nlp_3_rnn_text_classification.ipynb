{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch-nlp_3_rnn_text_classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "00U5-5uAbkD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm, tqdm_notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W_5e1bfhrZy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92f7aaa3-73c0-4b41-b70f-b46f7ec4bf6d"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsW7E1ZWiBwG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "outputId": "3d628361-7085-4aab-9f24-6fcd0287fd09"
      },
      "source": [
        "DATA_PATH = 'data/imdb_review.csv'\n",
        "if not Path(DATA_PATH).is_file():\n",
        "  gdd.download_file_from_google_drive(\n",
        "      file_id='1zfM5E6HvKIe7f3rEt1V2gBpw5QOSSKQz',\n",
        "      dest_path=DATA_PATH,\n",
        "  )"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1zfM5E6HvKIe7f3rEt1V2gBpw5QOSSKQz into data/imdb_review.csv... Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl4q4-cjiYr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sequences(Dataset):\n",
        "  def __init__(self, path, max_seq_len):\n",
        "    self.max_seq_len = max_seq_len\n",
        "    df = pd.read_csv(path)\n",
        "    print(df.shape)\n",
        "    vectorizer = CountVectorizer(stop_words='english', min_df=0.015)\n",
        "    vectorizer.fit(df.review.tolist())\n",
        "\n",
        "    self.token2idx = vectorizer.vocabulary_\n",
        "    self.token2idx['<PAD>'] = max(self.token2idx.values()) + 1\n",
        "\n",
        "    tokenizer = vectorizer.build_analyzer()\n",
        "    self.encode = lambda x: [self.token2idx[token] for token in tokenizer(x) if token in self.token2idx]\n",
        "    print(self.encode)\n",
        "    self.pad = lambda x: x + (max_seq_len - len(x)) * [self.token2idx['<PAD>']]\n",
        "    \n",
        "    sequences = [self.encode(sequence)[:max_seq_len] for sequence in df.review.tolist()]\n",
        "    print([self.encode(sequence)[:max_seq_len] for sequence in df.review.tolist()[0]])\n",
        "    sequences, self.labels = zip(*[(sequence, label) for sequence, label in zip(sequences, df.label.tolist()) if sequence])\n",
        "\n",
        "    self.sequences = [self.pad(sequence) for sequence in sequences]\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    assert len(self.sequences[i]) == self.max_seq_len\n",
        "    return self.sequences[i], self.labels[i]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLa1tG7InNhq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "3bf77619-8b76-4716-fced-6ec5e98e434a"
      },
      "source": [
        "dataset = Sequences(DATA_PATH, max_seq_len=128)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(62155, 2)\n",
            "<function Sequences.__init__.<locals>.<lambda> at 0x7f24cd3a0d90>\n",
            "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzIEOTyrnb75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d45a85a-f520-4a8d-fe8f-086622b4b54d"
      },
      "source": [
        "len(dataset.token2idx)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1104"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr-gTh0F2ON_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate(batch):\n",
        "  inputs = torch.LongTensor([item[0] for item in batch])\n",
        "  target = torch.FloatTensor([item[1] for item in batch])\n",
        "  return inputs, target\n",
        "\n",
        "batch_size = 2048\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWEMBQEO4rUK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c6b0754-2df6-4ce9-b015-394a75e9f7d8"
      },
      "source": [
        "import numpy as np\n",
        "np.array(train_loader.dataset[0]).shape, np.array(train_loader.dataset[0][0]).shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2,), (128,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn6ek32e3HoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      vocab_size,\n",
        "      batch_size,\n",
        "      embedding_dimension=100,\n",
        "      hidden_size=128,\n",
        "      n_layers=1,\n",
        "      device='cuda'\n",
        "  ):\n",
        "    super(RNN, self).__init__()\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    self.device = device\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.encoder = nn.Embedding(vocab_size, embedding_dimension)\n",
        "    self.rnn = nn.GRU(\n",
        "        embedding_dimension,\n",
        "        hidden_size,\n",
        "        num_layers=n_layers,\n",
        "        batch_first=True,\n",
        "    )\n",
        "    self.decoder = nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def init_hidden(self):\n",
        "    # print(torch.randn(self.n_layers, self.batch_size, self.hidden_size).to(self.device).shape) # torch.Size([1, 2048, 128])\n",
        "    return torch.randn(self.n_layers, self.batch_size, self.hidden_size).to(self.device)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    batch_size = inputs.size(0)\n",
        "    if batch_size != self.batch_size:\n",
        "      self.batch_size = batch_size\n",
        "\n",
        "    encoded = self.encoder(inputs)\n",
        "    # print(encoded.shape) # torch.Size([2048, 128, 100])\n",
        "    output, hidden = self.rnn(encoded, self.init_hidden())\n",
        "    # print(output.shape, output[:, :, -1].shape) # torch.Size([2048, 128, 128]) torch.Size([2048, 128])\n",
        "    output = self.decoder(output[:, :, -1]).squeeze()\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__a2mq2098IN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e3728ed0-0f97-4d1b-b12e-fd654d409240"
      },
      "source": [
        "model = RNN(\n",
        "    hidden_size=128,\n",
        "    vocab_size=len(dataset.token2idx),\n",
        "    device=device,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "model = model.to(device)\n",
        "model"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (encoder): Embedding(1104, 100)\n",
              "  (rnn): GRU(100, 128, batch_first=True)\n",
              "  (decoder): Linear(in_features=128, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxFVwd8J-CVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEkSvKVq-5Dx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cde73835-7057-4fcd-818c-4a535e329d30"
      },
      "source": [
        "model.train()\n",
        "train_losses = []\n",
        "for epoch in range(10):\n",
        "  losses = []\n",
        "  total = 0\n",
        "  for inputs, target in train_loader:\n",
        "    inputs, target = inputs.to(device), target.to(device)\n",
        "    \n",
        "    model.zero_grad()\n",
        "\n",
        "    output = model(inputs)\n",
        "\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Loss: {loss.item():.3f}')\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    total += 1\n",
        "\n",
        "  epoch_loss = sum(losses) / total\n",
        "  train_losses.append(epoch_loss)\n",
        "\n",
        "  print(f'Epoch #{epoch + 1}\\tTrain Loss: {epoch_loss:.3f}')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.594\n",
            "Loss: 0.515\n",
            "Loss: 0.459\n",
            "Loss: 0.417\n",
            "Loss: 0.374\n",
            "Loss: 0.347\n",
            "Loss: 1.332\n",
            "Loss: 1.397\n",
            "Loss: 1.413\n",
            "Loss: 1.408\n",
            "Loss: 1.403\n",
            "Loss: 1.355\n",
            "Loss: 0.431\n",
            "Loss: 0.340\n",
            "Loss: 0.347\n",
            "Loss: 0.352\n",
            "Loss: 0.357\n",
            "Loss: 0.346\n",
            "Loss: 1.162\n",
            "Loss: 1.286\n",
            "Loss: 1.286\n",
            "Loss: 1.282\n",
            "Loss: 1.237\n",
            "Loss: 1.211\n",
            "Loss: 0.854\n",
            "Loss: 0.738\n",
            "Loss: 0.744\n",
            "Loss: 0.708\n",
            "Loss: 0.723\n",
            "Loss: 0.720\n",
            "Loss: 0.721\n",
            "Epoch #1\tTrain Loss: 0.834\n",
            "Loss: 0.478\n",
            "Loss: 0.487\n",
            "Loss: 0.496\n",
            "Loss: 0.498\n",
            "Loss: 0.492\n",
            "Loss: 0.490\n",
            "Loss: 0.964\n",
            "Loss: 0.984\n",
            "Loss: 0.986\n",
            "Loss: 0.981\n",
            "Loss: 0.976\n",
            "Loss: 0.954\n",
            "Loss: 0.551\n",
            "Loss: 0.514\n",
            "Loss: 0.520\n",
            "Loss: 0.520\n",
            "Loss: 0.523\n",
            "Loss: 0.513\n",
            "Loss: 0.881\n",
            "Loss: 0.941\n",
            "Loss: 0.943\n",
            "Loss: 0.944\n",
            "Loss: 0.925\n",
            "Loss: 0.912\n",
            "Loss: 0.750\n",
            "Loss: 0.698\n",
            "Loss: 0.702\n",
            "Loss: 0.690\n",
            "Loss: 0.698\n",
            "Loss: 0.697\n",
            "Loss: 0.696\n",
            "Epoch #2\tTrain Loss: 0.723\n",
            "Loss: 0.599\n",
            "Loss: 0.603\n",
            "Loss: 0.605\n",
            "Loss: 0.600\n",
            "Loss: 0.593\n",
            "Loss: 0.584\n",
            "Loss: 0.825\n",
            "Loss: 0.847\n",
            "Loss: 0.851\n",
            "Loss: 0.852\n",
            "Loss: 0.849\n",
            "Loss: 0.837\n",
            "Loss: 0.607\n",
            "Loss: 0.585\n",
            "Loss: 0.588\n",
            "Loss: 0.584\n",
            "Loss: 0.583\n",
            "Loss: 0.574\n",
            "Loss: 0.804\n",
            "Loss: 0.844\n",
            "Loss: 0.844\n",
            "Loss: 0.837\n",
            "Loss: 0.812\n",
            "Loss: 0.789\n",
            "Loss: 0.709\n",
            "Loss: 0.690\n",
            "Loss: 0.693\n",
            "Loss: 0.689\n",
            "Loss: 0.691\n",
            "Loss: 0.690\n",
            "Loss: 0.688\n",
            "Epoch #3\tTrain Loss: 0.708\n",
            "Loss: 0.669\n",
            "Loss: 0.670\n",
            "Loss: 0.675\n",
            "Loss: 0.672\n",
            "Loss: 0.671\n",
            "Loss: 0.673\n",
            "Loss: 0.709\n",
            "Loss: 0.713\n",
            "Loss: 0.716\n",
            "Loss: 0.714\n",
            "Loss: 0.714\n",
            "Loss: 0.711\n",
            "Loss: 0.677\n",
            "Loss: 0.670\n",
            "Loss: 0.672\n",
            "Loss: 0.671\n",
            "Loss: 0.673\n",
            "Loss: 0.670\n",
            "Loss: 0.707\n",
            "Loss: 0.713\n",
            "Loss: 0.712\n",
            "Loss: 0.714\n",
            "Loss: 0.712\n",
            "Loss: 0.711\n",
            "Loss: 0.694\n",
            "Loss: 0.688\n",
            "Loss: 0.690\n",
            "Loss: 0.687\n",
            "Loss: 0.687\n",
            "Loss: 0.688\n",
            "Loss: 0.682\n",
            "Epoch #4\tTrain Loss: 0.691\n",
            "Loss: 0.677\n",
            "Loss: 0.677\n",
            "Loss: 0.680\n",
            "Loss: 0.675\n",
            "Loss: 0.671\n",
            "Loss: 0.670\n",
            "Loss: 0.706\n",
            "Loss: 0.713\n",
            "Loss: 0.717\n",
            "Loss: 0.714\n",
            "Loss: 0.714\n",
            "Loss: 0.710\n",
            "Loss: 0.667\n",
            "Loss: 0.662\n",
            "Loss: 0.663\n",
            "Loss: 0.662\n",
            "Loss: 0.661\n",
            "Loss: 0.654\n",
            "Loss: 0.706\n",
            "Loss: 0.715\n",
            "Loss: 0.718\n",
            "Loss: 0.717\n",
            "Loss: 0.712\n",
            "Loss: 0.705\n",
            "Loss: 0.681\n",
            "Loss: 0.675\n",
            "Loss: 0.676\n",
            "Loss: 0.673\n",
            "Loss: 0.671\n",
            "Loss: 0.672\n",
            "Loss: 0.662\n",
            "Epoch #5\tTrain Loss: 0.686\n",
            "Loss: 0.704\n",
            "Loss: 0.702\n",
            "Loss: 0.699\n",
            "Loss: 0.683\n",
            "Loss: 0.665\n",
            "Loss: 0.653\n",
            "Loss: 0.693\n",
            "Loss: 0.715\n",
            "Loss: 0.721\n",
            "Loss: 0.717\n",
            "Loss: 0.720\n",
            "Loss: 0.712\n",
            "Loss: 0.631\n",
            "Loss: 0.629\n",
            "Loss: 0.629\n",
            "Loss: 0.629\n",
            "Loss: 0.620\n",
            "Loss: 0.604\n",
            "Loss: 0.702\n",
            "Loss: 0.720\n",
            "Loss: 0.727\n",
            "Loss: 0.718\n",
            "Loss: 0.709\n",
            "Loss: 0.691\n",
            "Loss: 0.650\n",
            "Loss: 0.642\n",
            "Loss: 0.641\n",
            "Loss: 0.637\n",
            "Loss: 0.630\n",
            "Loss: 0.637\n",
            "Loss: 0.615\n",
            "Epoch #6\tTrain Loss: 0.672\n",
            "Loss: 0.723\n",
            "Loss: 0.718\n",
            "Loss: 0.703\n",
            "Loss: 0.678\n",
            "Loss: 0.637\n",
            "Loss: 0.612\n",
            "Loss: 0.655\n",
            "Loss: 0.702\n",
            "Loss: 0.708\n",
            "Loss: 0.702\n",
            "Loss: 0.709\n",
            "Loss: 0.691\n",
            "Loss: 0.563\n",
            "Loss: 0.570\n",
            "Loss: 0.568\n",
            "Loss: 0.576\n",
            "Loss: 0.551\n",
            "Loss: 0.524\n",
            "Loss: 0.664\n",
            "Loss: 0.689\n",
            "Loss: 0.695\n",
            "Loss: 0.679\n",
            "Loss: 0.661\n",
            "Loss: 0.620\n",
            "Loss: 0.580\n",
            "Loss: 0.579\n",
            "Loss: 0.575\n",
            "Loss: 0.582\n",
            "Loss: 0.568\n",
            "Loss: 0.592\n",
            "Loss: 0.557\n",
            "Epoch #7\tTrain Loss: 0.633\n",
            "Loss: 0.762\n",
            "Loss: 0.745\n",
            "Loss: 0.694\n",
            "Loss: 0.646\n",
            "Loss: 0.578\n",
            "Loss: 0.535\n",
            "Loss: 0.617\n",
            "Loss: 0.694\n",
            "Loss: 0.703\n",
            "Loss: 0.698\n",
            "Loss: 0.704\n",
            "Loss: 0.682\n",
            "Loss: 0.474\n",
            "Loss: 0.488\n",
            "Loss: 0.489\n",
            "Loss: 0.504\n",
            "Loss: 0.473\n",
            "Loss: 0.448\n",
            "Loss: 0.629\n",
            "Loss: 0.647\n",
            "Loss: 0.653\n",
            "Loss: 0.642\n",
            "Loss: 0.625\n",
            "Loss: 0.585\n",
            "Loss: 0.539\n",
            "Loss: 0.531\n",
            "Loss: 0.532\n",
            "Loss: 0.531\n",
            "Loss: 0.513\n",
            "Loss: 0.543\n",
            "Loss: 0.500\n",
            "Epoch #8\tTrain Loss: 0.594\n",
            "Loss: 0.654\n",
            "Loss: 0.658\n",
            "Loss: 0.621\n",
            "Loss: 0.592\n",
            "Loss: 0.534\n",
            "Loss: 0.494\n",
            "Loss: 0.556\n",
            "Loss: 0.640\n",
            "Loss: 0.632\n",
            "Loss: 0.624\n",
            "Loss: 0.626\n",
            "Loss: 0.583\n",
            "Loss: 0.442\n",
            "Loss: 0.480\n",
            "Loss: 0.487\n",
            "Loss: 0.508\n",
            "Loss: 0.463\n",
            "Loss: 0.435\n",
            "Loss: 0.533\n",
            "Loss: 0.532\n",
            "Loss: 0.537\n",
            "Loss: 0.535\n",
            "Loss: 0.513\n",
            "Loss: 0.472\n",
            "Loss: 0.478\n",
            "Loss: 0.484\n",
            "Loss: 0.488\n",
            "Loss: 0.496\n",
            "Loss: 0.472\n",
            "Loss: 0.510\n",
            "Loss: 0.455\n",
            "Epoch #9\tTrain Loss: 0.533\n",
            "Loss: 0.640\n",
            "Loss: 0.641\n",
            "Loss: 0.580\n",
            "Loss: 0.542\n",
            "Loss: 0.473\n",
            "Loss: 0.425\n",
            "Loss: 0.542\n",
            "Loss: 0.635\n",
            "Loss: 0.631\n",
            "Loss: 0.636\n",
            "Loss: 0.646\n",
            "Loss: 0.602\n",
            "Loss: 0.376\n",
            "Loss: 0.400\n",
            "Loss: 0.413\n",
            "Loss: 0.434\n",
            "Loss: 0.392\n",
            "Loss: 0.376\n",
            "Loss: 0.530\n",
            "Loss: 0.527\n",
            "Loss: 0.527\n",
            "Loss: 0.529\n",
            "Loss: 0.503\n",
            "Loss: 0.468\n",
            "Loss: 0.458\n",
            "Loss: 0.457\n",
            "Loss: 0.459\n",
            "Loss: 0.466\n",
            "Loss: 0.442\n",
            "Loss: 0.480\n",
            "Loss: 0.424\n",
            "Epoch #10\tTrain Loss: 0.505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkpcTzpHAiJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_sentiment(text):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    test_vector = torch.LongTensor([dataset.pad(dataset.encode(text))]).to(device)\n",
        "\n",
        "    output = model(test_vector)\n",
        "    prediction = torch.sigmoid(output).item()\n",
        "\n",
        "    if prediction > 0.5:\n",
        "      print(f'{prediction:0.3}: Positive sentiment')\n",
        "    else:\n",
        "      print(f'{prediction:0.3}: Negative sentiment')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sCdDYMFGJFO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4076f849-47c3-4905-91f6-6eb0bc42077c"
      },
      "source": [
        "test_text = \"\"\"\n",
        "This poor excuse for a movie is terrible. It has been 'so good it's bad' for a\n",
        "while, and the high ratings are a good form of sarcasm, I have to admit. But\n",
        "now it has to stop. Technically inept, spoon-feeding mundane messages with the\n",
        "artistic weight of an eighties' commercial, hypocritical to say the least, it\n",
        "deserves to fall into oblivion. Mr. Derek, I hope you realize you are like that\n",
        "weird friend that everybody know is lame, but out of kindness and Christian\n",
        "duty is treated like he's cool or something. That works if you are a good\n",
        "decent human being, not if you are a horrible arrogant bully like you are. Yes,\n",
        "Mr. 'Daddy' Derek will end on the history books of the internet for being a\n",
        "delusional sour old man who thinks to be a good example for kids, but actually\n",
        "has a poster of Kim Jong-Un in his closet. Destroy this movie if you all have a\n",
        "conscience, as I hope IHE and all other youtube channel force-closed by Derek\n",
        "out of SPITE would destroy him in the courts.This poor excuse for a movie is\n",
        "terrible. It has been 'so good it's bad' for a while, and the high ratings are\n",
        "a good form of sarcasm, I have to admit. But now it has to stop. Technically\n",
        "inept, spoon-feeding mundane messages with the artistic weight of an eighties'\n",
        "commercial, hypocritical to say the least, it deserves to fall into oblivion.\n",
        "Mr. Derek, I hope you realize you are like that weird friend that everybody\n",
        "know is lame, but out of kindness and Christian duty is treated like he's cool\n",
        "or something. That works if you are a good decent human being, not if you are a\n",
        "horrible arrogant bully like you are. Yes, Mr. 'Daddy' Derek will end on the\n",
        "history books of the internet for being a delusional sour old man who thinks to\n",
        "be a good example for kids, but actually has a poster of Kim Jong-Un in his\n",
        "closet. Destroy this movie if you all have a conscience, as I hope IHE and all\n",
        "other youtube channel force-closed by Derek out of SPITE would destroy him in\n",
        "the courts.\n",
        "\"\"\"\n",
        "predict_sentiment(test_text)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.518: Positive sentiment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ8xgZb0GK06",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62c71561-9239-4145-c6da-05f82135366d"
      },
      "source": [
        "test_text = \"\"\"\n",
        "Cool Cat Saves The Kids is a symbolic masterpiece directed by Derek Savage that\n",
        "is not only satirical in the way it makes fun of the media and politics, but in\n",
        "the way in questions as how we humans live life and how society tells us to\n",
        "live life.\n",
        "\n",
        "Before I get into those details, I wanna talk about the special effects in this\n",
        "film. They are ASTONISHING, and it shocks me that Cool Cat Saves The Kids got\n",
        "snubbed by the Oscars for Best Special Effects. This film makes 2001 look like\n",
        "garbage, and the directing in this film makes Stanley Kubrick look like the\n",
        "worst director ever. You know what other film did that? Birdemic: Shock and\n",
        "Terror. Both of these films are masterpieces, but if I had to choose my\n",
        "favorite out of the 2, I would have to go with Cool Cat Saves The Kids. It is\n",
        "now my 10th favorite film of all time.\n",
        "\n",
        "Now, lets get into the symbolism: So you might be asking yourself, Why is Cool\n",
        "Cat Orange? Well, I can easily explain. Orange is a color. Orange is also a\n",
        "fruit, and its a very good fruit. You know what else is good? Good behavior.\n",
        "What behavior does Cool Cat have? He has good behavior. This cannot be a\n",
        "coincidence, since cool cat has good behavior in the film.\n",
        "\n",
        "Now, why is Butch The Bully fat? Well, fat means your wide. You wanna know who\n",
        "was wide? Hitler. Nuff said this cannot be a coincidence.\n",
        "\n",
        "Why does Erik Estrada suspect Butch The Bully to be a bully? Well look at it\n",
        "this way. What color of a shirt was Butchy wearing when he walks into the area?\n",
        "I don't know, its looks like dark purple/dark blue. Why rhymes with dark? Mark.\n",
        "Mark is that guy from the Room. The Room is the best movie of all time. What is\n",
        "the opposite of best? Worst. This is how Erik knew Butch was a bully.\n",
        "\n",
        "and finally, how come Vivica A. Fox isn't having a successful career after\n",
        "making Kill Bill.\n",
        "\n",
        "I actually can't answer that question.\n",
        "\n",
        "Well thanks for reading my review.\n",
        "\"\"\"\n",
        "predict_sentiment(test_text)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.785: Positive sentiment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whMjvlZEGOMe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f494de8-2232-45bb-e681-b8e524be38e8"
      },
      "source": [
        "test_text = \"\"\"\n",
        "Don't let any bullies out there try and shape your judgment on this gem of a\n",
        "title.\n",
        "\n",
        "Some people really don't have anything better to do, except trash a great movie\n",
        "with annoying 1-star votes and spread lies on the Internet about how \"dumb\"\n",
        "Cool Cat is.\n",
        "\n",
        "I wouldn't be surprised to learn if much of the unwarranted negativity hurled\n",
        "at this movie is coming from people who haven't even watched this movie for\n",
        "themselves in the first place. Those people are no worse than the Butch the\n",
        "Bully, the film's repulsive antagonist.\n",
        "\n",
        "As it just so happens, one of the main points of \"Cool Cat Saves the Kids\" is\n",
        "in addressing the attitudes of mean naysayers who try to demean others who\n",
        "strive to bring good attitudes and fun vibes into people's lives. The message\n",
        "to be learned here is that if one is friendly and good to others, the world is\n",
        "friendly and good to one in return, and that is cool. Conversely, if one is\n",
        "miserable and leaving 1-star votes on IMDb, one is alone and doesn't have any\n",
        "friends at all. Ain't that the truth?\n",
        "\n",
        "The world has uncovered a great, new, young filmmaking talent in \"Cool Cat\"\n",
        "creator Derek Savage, and I sure hope that this is only the first of many\n",
        "amazing films and stories that the world has yet to appreciate.\n",
        "\n",
        "If you are a cool person who likes to have lots of fun, I guarantee that this\n",
        "is a movie with charm that will uplift your spirits and reaffirm your positive\n",
        "attitudes towards life.\n",
        "\"\"\"\n",
        "predict_sentiment(test_text)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.748: Positive sentiment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm7s9t_CGSP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}